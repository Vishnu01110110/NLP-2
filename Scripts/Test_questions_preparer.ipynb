{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGuDpR2SCaTh",
        "outputId": "fa6a4aab-a127-4b8c-87ee-e1ba1b99e600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/NLP-2-main.zip\"\n",
        "\n",
        "extract_path = \"/content/NLP-2-extracted\"\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2LsSj-ZCkaG",
        "outputId": "05ae7e19-6aad-48ec-bf1d-f09cb0cbf610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/NLP-2-extracted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Contents of the extracted folder:\")\n",
        "for item in os.listdir(extract_path):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ygXZNESFwIN",
        "outputId": "45bf40cd-50da-4dea-81ee-064d5ae887f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the extracted folder:\n",
            "NLP-2-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaII-Ar8HEg0",
        "outputId": "4ab48f32-7218-41b7-aac3-b08ef0005310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Downloading openai-1.52.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.9/386.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jiter, h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.52.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "# from getpass import getpass\n",
        "\n",
        "api_key = 'sk-9zdEnnpoNRxNaCvWJS3'\n",
        "\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://cmu.litellm.ai\"\n",
        ")\n",
        "\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-4o\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"\\nEnter your question (or 'quit' to exit): \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "    response = get_completion(user_input)\n",
        "    print(\"\\nResponse:\", response)\n",
        "\n",
        "print(\"Thank you for using the LTI LiteLLM API Proxy!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV_DLo2dF71n",
        "outputId": "df3ffa73-ddec-494c-e099-1aed39b0c672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today?\n",
            "\n",
            "Enter your question (or 'quit' to exit): Hi\n",
            "\n",
            "Response: Hello! How can I assist you today?\n",
            "\n",
            "Enter your question (or 'quit' to exit): quit\n",
            "Thank you for using the LTI LiteLLM API Proxy!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "api_key = 'sk-9zdEnnpoNRxNaCvWJS36Vg'\n",
        "client = openai.OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://cmu.litellm.ai\"\n",
        ")\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-4o\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def chunk_text(text, chunk_size=10000):\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) <= chunk_size:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n",
        "\n",
        "def generate_qa_pair(chunk):\n",
        "    prompt = f\"\"\"Based on the following text, generate a single-line question and its answer. The question should be specific and answerable from the given text.\n",
        "\n",
        "Example:\n",
        "Text: The Eiffel Tower, located in Paris, France, was completed in 1889. It stands at a height of 324 meters and was the tallest man-made structure in the world for 41 years until the Chrysler Building in New York City was built in 1930.\n",
        "Q: In what year was the Eiffel Tower completed?\n",
        "A: The Eiffel Tower was completed in 1889.\n",
        "\n",
        "Now, generate a question and answer for the following text:\n",
        "{chunk}\"\"\"\n",
        "    response = get_completion(prompt)\n",
        "    qa_pair = response.split('\\n', 1)\n",
        "    if len(qa_pair) == 2:\n",
        "        return qa_pair[0].strip(), qa_pair[1].strip()\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "def process_documents(folder_path, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.txt'):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                print(f\"Processing: {filename}\")\n",
        "                with open(file_path, 'r', encoding='utf-8') as infile:\n",
        "                    content = infile.read()\n",
        "                chunks = chunk_text(content)\n",
        "                outfile.write(f\"File: {filename}\\n\\n\")\n",
        "                for chunk in chunks:\n",
        "                    question, answer = generate_qa_pair(chunk)\n",
        "                    if question and answer:\n",
        "                        outfile.write(f\"Q: {question}\\n\")\n",
        "                        outfile.write(f\"A: {answer}\\n\\n\")\n",
        "                outfile.write(\"=\"*50 + \"\\n\\n\")\n",
        "                print(f\"Completed: {filename}\")\n",
        "\n",
        "folder_path = '/content/NLP-2-extracted/NLP-2-main/raw_data/Sports'\n",
        "output_file = '/content/test_set_sports.txt'\n",
        "\n",
        "print('Process started')\n",
        "process_documents(folder_path, output_file)\n",
        "print(f\"Q&A pairs have been generated and saved to {output_file}\")\n",
        "print('Process completed')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNSBm6R5HTrP",
        "outputId": "517fbfcf-4685-480a-9f7b-ff8555458989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process started\n",
            "Processing: penguins.txt\n",
            "Completed: penguins.txt\n",
            "Processing: steelers.txt\n",
            "Completed: steelers.txt\n",
            "Processing: visitpittsburgh_copy.txt\n",
            "Completed: visitpittsburgh_copy.txt\n",
            "Processing: pirates.txt\n",
            "Completed: pirates.txt\n",
            "Q&A pairs have been generated and saved to /content/test_set_sports.txt\n",
            "Process completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aDJu4rrJKb3v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}